{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 📚 Prerequisites\n",
    "\n",
    "Before running this notebook, ensure you have provisioned your Azure resources, configured your environment variables (Search endpoint, key, and index name), and set up a Conda environment to manage dependencies. Refer to [REQUIREMENTS.md](REQUIREMENTS.md) for full setup instructions.\n",
    "\n",
    "## 📋 Table of Contents\n",
    "\n",
    "This notebook walks through **two approaches for indexing policy documents stored in Azure Blob Storage**, using either the **Push SDK** or the **native Azure AI Search Indexer**.\n",
    "\n",
    "#### 1. [**Indexing Policy Documents from Blob Storage Using the SDK (Push Model)**](#index-using-push-sdk)\n",
    "\n",
    "In this approach, we build a fully controlled indexing pipeline using the Azure SDK for Python.\n",
    "\n",
    "You will learn how to:\n",
    "\n",
    "- Load PDF or text-based policy documents directly from Blob Storage\n",
    "- Preprocess and chunk content for retrieval\n",
    "- Generate embeddings using Azure OpenAI\n",
    "- Upload documents, metadata, and vector embeddings into Azure AI Search using the `SearchClient`\n",
    "\n",
    "This method gives you **maximum flexibility** and is ideal when you need to integrate your own preprocessing logic, custom chunking strategies, or real-time updates.\n",
    "\n",
    "#### 2. [**Indexing Policy Documents Using Azure AI Search Indexers with Custom Skillsets**](#index-using-indexer)\n",
    "\n",
    "This approach uses the built-in **Azure Search Indexer** to automatically crawl documents from Blob Storage and enrich them using a **custom skillset**.\n",
    "\n",
    "You will learn how to:\n",
    "\n",
    "- Connect Blob Storage as a data source\n",
    "- Define a skillset that performs OCR, embedding, or text extraction\n",
    "- Automatically map enriched fields into your Azure Search index\n",
    "\n",
    "This method is best when you want to **automate the ingestion pipeline** with minimal custom code, leveraging Azure's low-code enrichment platform.\n",
    "\n",
    "> By the end of this notebook, you’ll understand how to build both a **fully customized ingestion flow** using the SDK and a **scalable low-code pipeline** using native indexers with skillsets — both optimized for retrieving policy content stored in Blob Storage.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Directory changed to c:\\Users\\pablosal\\Desktop\\aihlsignited-medindexer\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from tenacity import retry, wait_random_exponential, stop_after_attempt\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "import json\n",
    "import copy\n",
    "from azure.core.credentials import AzureKeyCredential\n",
    "from azure.search.documents import SearchClient\n",
    "from azure.search.documents.indexes import SearchIndexClient\n",
    "from azure.search.documents.indexes.models import (\n",
    "    ExhaustiveKnnAlgorithmConfiguration,\n",
    "    ExhaustiveKnnParameters,\n",
    "    SearchIndex,\n",
    "    SearchField,\n",
    "    SearchFieldDataType,\n",
    "    SimpleField,\n",
    "    SearchableField,\n",
    "    SearchIndex,\n",
    "    SemanticConfiguration,\n",
    "    SemanticPrioritizedFields,\n",
    "    SemanticField,\n",
    "    SearchField,\n",
    "    VectorSearch,\n",
    "    SemanticSearch,\n",
    "    HnswAlgorithmConfiguration,\n",
    "    HnswParameters,\n",
    "    VectorSearch,\n",
    "    VectorSearchAlgorithmKind,\n",
    "    VectorSearchProfile,\n",
    "    SearchIndex,\n",
    "    SearchField,\n",
    "    SearchFieldDataType,\n",
    "    SimpleField,\n",
    "    SearchableField,\n",
    "    VectorSearch,\n",
    "    ExhaustiveKnnParameters,\n",
    "    SearchIndex,\n",
    "    SearchField,\n",
    "    SearchFieldDataType,\n",
    "    SimpleField,\n",
    "    SearchableField,\n",
    "    SearchIndex,\n",
    "    SemanticConfiguration,\n",
    "    SemanticField,\n",
    "    SearchField,\n",
    "    VectorSearch,\n",
    "    HnswParameters,\n",
    "    VectorSearch,\n",
    "    VectorSearchAlgorithmKind,\n",
    "    VectorSearchAlgorithmMetric,\n",
    "    VectorSearchProfile,\n",
    ")\n",
    "\n",
    "# Load environment variables from .env file\n",
    "load_dotenv()\n",
    "\n",
    "# Define the target directory\n",
    "target_directory = os.getcwd()  # Get the current working directory\n",
    "\n",
    "# Move one directory back\n",
    "parent_directory = os.path.dirname(target_directory)\n",
    "\n",
    "# Check if the parent directory exists\n",
    "if os.path.exists(parent_directory):\n",
    "    # Change the current working directory to the parent directory\n",
    "    os.chdir(parent_directory)\n",
    "    print(f\"Directory changed to {os.getcwd()}\")\n",
    "else:\n",
    "    print(f\"Parent directory {parent_directory} does not exist.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Indexing Policy Documents from Blob Storage Using the SDK (Push Model)**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **1. Use Document Intelligence to parse and read the PDF into text (Markdown)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-03-30 21:08:04,990 - micro - MainProcess - INFO     Container 'pre-auth-policies' already exists. (blob_helper.py:_create_container_if_not_exists:89)\n"
     ]
    }
   ],
   "source": [
    "from src.documentintelligence.document_intelligence_helper import AzureDocumentIntelligenceManager\n",
    "\n",
    "text_extractor = AzureDocumentIntelligenceManager()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-03-30 21:08:05,077 - micro - MainProcess - INFO     Blob URL detected. Extracting content. (document_intelligence_helper.py:analyze_document:78)\n",
      "2025-03-30 21:08:05,385 - micro - MainProcess - INFO     Downloaded blob 'policies_ocr/001.pdf' as bytes. (blob_helper.py:download_blob_to_bytes:311)\n"
     ]
    }
   ],
   "source": [
    "policy_raw_text_markdown = text_extractor.analyze_document(document_input=\"https://storageaeastusfactory.blob.core.windows.net/pre-auth-policies/policies_ocr/001.pdf\", \n",
    "                                model_type=\"prebuilt-layout\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<figure>\\n\\ncigna\\nhealthcare\\n\\n</figure>\\n\\n\\n# PRIOR AUTHORIZATION POLICY\\n\\nPOLICY:\\n\\nInflammatory Conditio'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "policy_raw_text_markdown.content[:100]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **2. Extract Metadata**\n",
    "\n",
    "- **Policy Name**: Extracted from document title or headers (e.g., \"Antiseizure Medications – Epidiolex Prior Authorization Policy\").\n",
    "- **Payer Name**: Identifies which insurance company issued the policy (e.g., Cigna, UnitedHealthcare).\n",
    "- **Drug Name(s)**: The medications referenced in the policy (e.g., Epidiolex, Dupixent).\n",
    "- **Medical Specialties Involved**: Identifies if a policy is specific to Rheumatology, Neurology, Oncology, etc.\n",
    "- **Indications & Diseases Covered**: Disease categories linked to a policy (e.g., Crohn’s disease, epilepsy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from typing import List, Dict, Optional\n",
    "from pydantic import BaseModel\n",
    "import openai\n",
    "import os\n",
    "import time\n",
    "import requests\n",
    "from azure.core.credentials import AzureKeyCredential\n",
    "from utils.ml_logging import get_logger\n",
    "\n",
    "logger = get_logger()\n",
    "\n",
    "# --------------------------------------------------------------------------\n",
    "# Configure the Azure OpenAI client using key-based authentication.\n",
    "# --------------------------------------------------------------------------\n",
    "client = openai.AzureOpenAI(\n",
    "    api_version=\"2024-12-01-preview\",\n",
    "    azure_endpoint=\"https://pablo-m2areked-westeurope.cognitiveservices.azure.com\",\n",
    "    azure_deployment=\"gpt-4o-structured-outputs\",\n",
    "    api_key=os.getenv(\"AZURE_OPENAI_KEY_StructuredOutputs\")\n",
    "    )  # Ensure this env var is set.\n",
    "\n",
    "model_name = \"gpt-4o-structured-outputs\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----------------- DEFINE POLICY METADATA MODEL -----------------\n",
    "class PolicyMetadata(BaseModel):\n",
    "    policy_name: str\n",
    "    payer_name: str\n",
    "    drug_names: List[str]\n",
    "    medical_specialties: List[str]\n",
    "    indications_diseases: List[str]\n",
    "    covered_diseases_icd_codes: Optional[List[str]]\n",
    "    covered_drug_codes: Optional[List[str]]\n",
    "\n",
    "    class Config:\n",
    "        extra = \"forbid\"\n",
    "\n",
    "# ----------------- ICD-10 LOOKUP FUNCTION -----------------\n",
    "def lookup_icd_codes_for_disease(disease: str, max_retries: int = 2) -> List[str]:\n",
    "    \"\"\"Fetches up to three ICD-10 codes for a given disease with retries on failure.\"\"\"\n",
    "\n",
    "    if not isinstance(disease, str) or not disease.strip():\n",
    "        logger.error(\"Invalid disease parameter provided.\")\n",
    "        return []\n",
    "\n",
    "    url = \"https://clinicaltables.nlm.nih.gov/api/icd10cm/v3/search\"\n",
    "    params = {\"sf\": \"code,name\", \"terms\": disease, \"maxList\": 3}\n",
    "\n",
    "    for attempt in range(max_retries + 1):\n",
    "        try:\n",
    "            logger.info(f\"[Attempt {attempt+1}] Fetching ICD-10 codes for disease: '{disease}' with params: {params}\")\n",
    "            resp = requests.get(url, params=params, timeout=10)\n",
    "            resp.raise_for_status()\n",
    "\n",
    "            if 'application/json' not in resp.headers.get('Content-Type', ''):\n",
    "                logger.error(f\"Unexpected content type: {resp.headers.get('Content-Type')}\")\n",
    "                return []\n",
    "\n",
    "            data = resp.json()\n",
    "            logger.debug(f\"Full ICD-10 API Response: {data}\")\n",
    "\n",
    "            # ✅ Extract only the ICD-10 codes and ensure they are valid\n",
    "            icd_codes = [item for item in data[1] if isinstance(item, str) and len(item) > 3]\n",
    "\n",
    "            if icd_codes:\n",
    "                logger.info(f\"ICD-10 Codes Found for '{disease}': {icd_codes}\")\n",
    "                return icd_codes\n",
    "            else:\n",
    "                logger.warning(f\"No valid ICD-10 codes found for '{disease}'.\")\n",
    "                return []\n",
    "\n",
    "        except requests.RequestException as e:\n",
    "            logger.error(f\"ICD lookup failed for '{disease}' on attempt {attempt+1}: {e}\")\n",
    "            if attempt < max_retries:\n",
    "                logger.info(f\"Retrying ICD lookup for '{disease}'...\")\n",
    "                time.sleep(2)  # Wait before retrying\n",
    "\n",
    "    logger.error(f\"ICD lookup ultimately failed for '{disease}' after {max_retries+1} attempts.\")\n",
    "    return []\n",
    "\n",
    "\n",
    "# ----------------- RXNORM LOOKUP FUNCTION -----------------\n",
    "def lookup_drug_details(drug: str, max_retries: int = 2) -> List[str]:\n",
    "    \"\"\"Fetches RxNorm IDs for TAH-validated drugs with retries on failure.\"\"\"\n",
    "    \n",
    "    if not isinstance(drug, str) or not drug.strip():\n",
    "        logger.error(\"Invalid drug parameter provided.\")\n",
    "        return []\n",
    "\n",
    "    url = \"https://rxnav.nlm.nih.gov/REST/rxcui.json\"\n",
    "    params = {\"name\": drug}\n",
    "\n",
    "    for attempt in range(max_retries + 1):\n",
    "        try:\n",
    "            logger.info(f\"[Attempt {attempt+1}] Fetching RxNorm ID for drug: '{drug}' with params: {params}\")\n",
    "            resp = requests.get(url, params=params, timeout=10)\n",
    "            resp.raise_for_status()\n",
    "\n",
    "            if 'application/json' not in resp.headers.get('Content-Type', ''):\n",
    "                logger.error(f\"Unexpected content type: {resp.headers.get('Content-Type')}\")\n",
    "                return []\n",
    "\n",
    "            data = resp.json()\n",
    "            logger.debug(f\"Full RxNorm API Response: {data}\")\n",
    "\n",
    "            # Extract only the RxNorm IDs\n",
    "            rxnorm_ids = data.get(\"idGroup\", {}).get(\"rxnormId\", [])\n",
    "\n",
    "            if rxnorm_ids:\n",
    "                logger.info(f\"RxNorm IDs Found for '{drug}': {rxnorm_ids}\")\n",
    "                return rxnorm_ids\n",
    "            else:\n",
    "                logger.warning(f\"No RxNorm ID found for '{drug}'.\")\n",
    "                return []\n",
    "\n",
    "        except requests.RequestException as e:\n",
    "            logger.error(f\"Drug lookup failed for '{drug}' on attempt {attempt+1}: {e}\")\n",
    "            if attempt < max_retries:\n",
    "                logger.info(f\"Retrying RxNorm lookup for '{drug}'...\")\n",
    "                time.sleep(2)  # Wait before retrying\n",
    "\n",
    "    logger.error(f\"RxNorm lookup ultimately failed for '{drug}' after {max_retries+1} attempts.\")\n",
    "    return []\n",
    "\n",
    "\n",
    "# ----------------- FINAL ENRICHMENT FUNCTION -----------------\n",
    "def enrich_metadata(metadata: PolicyMetadata) -> PolicyMetadata:\n",
    "    \"\"\"Enrich extracted metadata with ICD-10 and RxNorm codes, only for TAH-validated terms.\"\"\"\n",
    "    enriched = metadata.model_copy()\n",
    "    enriched.covered_diseases_icd_codes = [\n",
    "        code for disease in metadata.indications_diseases\n",
    "        for code in lookup_icd_codes_for_disease(disease)\n",
    "    ]\n",
    "    enriched.covered_drug_codes = [\n",
    "        code for drug in metadata.drug_names\n",
    "        for code in lookup_drug_details(drug)\n",
    "    ]\n",
    "    return enriched\n",
    "# --------------------------------------------------------------------------\n",
    "# Extraction Phase: Use Azure OpenAI to parse policy text into the above schema.\n",
    "# --------------------------------------------------------------------------\n",
    "# Optimized function for metadata extraction\n",
    "def extract_policy_metadata(policy_text: str) -> PolicyMetadata:\n",
    "    messages = [\n",
    "        {\n",
    "            \"role\": \"system\",\n",
    "            \"content\": (\n",
    "                \"You are an advanced AI system specializing in extracting **structured metadata** from clinical policy documents. \"\n",
    "                \"Your goal is to achieve **100% accuracy** by leveraging **Tree of Thought reasoning, multi-step validation techniques, \"\n",
    "                \"and automated normalization of payer names, drug names, and medical conditions**.\\n\\n\"\n",
    "\n",
    "                \"### **1️⃣ Extract Policy Name (`policy_name`)** 📄\\n\"\n",
    "                \"- **Identify the official policy title** from document **headers, footers, or the first paragraph**.\\n\"\n",
    "                \"- **Ensure Standard Formatting:** Convert extracted titles into a **consistent naming format**.\\n\"\n",
    "                \"  - **Example Input (Raw OCR Extracted Text):**\\n\"\n",
    "                \"    - 'DUPIXENT (DUPILUMAB) - PRIOR AUTHORIZATION POLICY'\\n\"\n",
    "                \"    - 'Anthem BCBS Policy 2024: Prior Authorization - Dupixent'\\n\"\n",
    "                \"  - **Expected Standard Output:**\\n\"\n",
    "                \"    - 'Dupixent Prior Authorization Policy'\\n\\n\"\n",
    "\n",
    "                \"2️⃣ **Payer Name (payer_name) - AUTOMATIC NORMALIZATION ENABLED:**\\n\"\n",
    "                \"   - Locate in **headers, footers, disclaimers, or embedded watermarks**.\\n\"\n",
    "                \"   - Normalize using the following standardized mapping:\\n\"\n",
    "                \"     - 'Cigna', 'Cigna Healthcare', 'Cigna Corp' → 'Cigna'\\n\"\n",
    "                \"     - 'Humana', 'Humana Inc', 'Humana Health Plan' → 'Humana'\\n\"\n",
    "                \"     - 'United Healthcare', 'United Health Care', 'UHC' → 'UnitedHealthcare'\\n\"\n",
    "                \"     - 'Anthem Blue Cross Blue Shield' → 'Anthem BCBS'\\n\"\n",
    "                \"     - 'Blue Cross Blue Shield' → 'BCBS'\\n\"\n",
    "                \"     - 'Kaiser Permanente' → 'Kaiser Permanente'\\n\"\n",
    "                \"     - 'Aetna', 'Aetna Health Inc', 'Aetna Insurance' → 'Aetna'\\n\"\n",
    "                \"     - 'WellCare Health Plans' → 'WellCare'\\n\"\n",
    "                \"     - 'Medicare Advantage' → 'Medicare'\\n\"\n",
    "                \"     - 'Medicaid' → 'Medicaid'\\n\"\n",
    "                \"     - 'MVP Health Care' → 'MVP HealthCare'\\n\"\n",
    "                \"     - 'HealthFirst' → 'HealthFirst'\\n\"\n",
    "                \"     - 'Molina Healthcare' → 'Molina Healthcare'\\n\"\n",
    "                \"     - 'Centene Corporation' → 'Centene'\\n\"\n",
    "                \"     - 'Blue Shield of California' → 'Blue Shield of California'\\n\"\n",
    "                \"     - 'Empire Blue Cross Blue Shield' → 'Empire BCBS'\\n\"\n",
    "                \"     - 'Horizon Blue Cross Blue Shield' → 'Horizon BCBS'\\n\"\n",
    "\n",
    "                \"3️⃣ **Drug Names (drug_names) - AUTOMATIC NORMALIZATION ENABLED:**\\n\"\n",
    "                \"   - Extract **all medications mentioned in the policy**.\\n\"\n",
    "                \"   - Include **both brand and generic names**.\\n\"\n",
    "                \"   - Normalize using **RxNorm drug database standards**.\\n\"\n",
    "\n",
    "                \"4️⃣ **Medical Specialties (medical_specialties):**\\n\"\n",
    "                \"   - Identify relevant **clinical specialties** (e.g., Neurology, Rheumatology, Oncology).\\n\"\n",
    "                \"   - Cross-check with **medical board designations** to avoid ambiguity.\\n\"\n",
    "\n",
    "                \"5️⃣ **Indications & Diseases (indications_diseases) - AUTOMATIC NORMALIZATION ENABLED:**\\n\"\n",
    "                \"   - Extract **every disease, condition, or indication mentioned**.\\n\"\n",
    "                \"   - Normalize to **ICD-10 classification**.\\n\"\n",
    "                \"   - Check **eligibility, exclusions, and coverage sections** for additional conditions.\\n\\n\"\n",
    "\n",
    "                \"### **General Guidelines:**\\n\"\n",
    "                \"- If a field is missing, return an **empty string (`\"\"`)** for text fields or an **empty list (`[]`)** for arrays.\\n\"\n",
    "                \"- Strictly **follow the JSON schema** without adding extra keys.\\n\"\n",
    "                \"- **Cross-validate extracted information** across multiple sections to prevent errors.\\n\"\n",
    "            )\n",
    "        },\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": (\n",
    "                \"Extract and normalize structured metadata from the following insurance policy document:\\n\\n\"\n",
    "                f\"{policy_text}\\n\\n\"\n",
    "                \"### **Output Schema (Strict JSON Format):**\\n\"\n",
    "                \"{\\n\"\n",
    "                '  \"policy_name\": \"The official title of the prior authorization policy.\",\\n'\n",
    "                '  \"payer_name\": \"The name of the insurance company, automatically normalized.\",\\n'\n",
    "                '  \"drug_names\": [\"List of all referenced drugs, including brand and generic, automatically normalized.\"],\\n'\n",
    "                '  \"medical_specialties\": [\"List of relevant clinical specialties (e.g., Neurology, Oncology).\"],\\n'\n",
    "                '  \"indications_diseases\": [\"List of all conditions covered under this policy, normalized to ICD-10 standards.\"]\\n'\n",
    "                \"}\"\n",
    "            )\n",
    "        }\n",
    "    ]\n",
    "\n",
    "    try:\n",
    "        response = client.beta.chat.completions.parse(\n",
    "            model=model_name,\n",
    "            messages=messages,\n",
    "            response_format=PolicyMetadata\n",
    "        )\n",
    "        metadata = response.choices[0].message.parsed\n",
    "\n",
    "        # Enrich metadata with ICD-10 and RxNorm codes\n",
    "        enriched_metadata = enrich_metadata(metadata)\n",
    "        enriched = enriched_metadata.model_dump()\n",
    "\n",
    "        return enriched\n",
    "    \n",
    "    except Exception as e:\n",
    "        logger.error(\"Error during extraction: %s\", e)\n",
    "        raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-03-30 21:11:22,810 - micro - MainProcess - INFO     [Attempt 1] Fetching ICD-10 codes for disease: 'Ankylosing Spondylitis' with params: {'sf': 'code,name', 'terms': 'Ankylosing Spondylitis', 'maxList': 3} (1126985018.py:lookup_icd_codes_for_disease:27)\n",
      "2025-03-30 21:11:23,190 - micro - MainProcess - INFO     ICD-10 Codes Found for 'Ankylosing Spondylitis': ['M08.1', 'M45.6', 'M45.2'] (1126985018.py:lookup_icd_codes_for_disease:42)\n",
      "2025-03-30 21:11:23,193 - micro - MainProcess - INFO     [Attempt 1] Fetching ICD-10 codes for disease: 'Crohn's Disease' with params: {'sf': 'code,name', 'terms': \"Crohn's Disease\", 'maxList': 3} (1126985018.py:lookup_icd_codes_for_disease:27)\n",
      "2025-03-30 21:11:23,351 - micro - MainProcess - INFO     ICD-10 Codes Found for 'Crohn's Disease': ['K50.90', 'K50.913', 'K50.914'] (1126985018.py:lookup_icd_codes_for_disease:42)\n",
      "2025-03-30 21:11:23,351 - micro - MainProcess - INFO     [Attempt 1] Fetching ICD-10 codes for disease: 'Hidradenitis Suppurativa' with params: {'sf': 'code,name', 'terms': 'Hidradenitis Suppurativa', 'maxList': 3} (1126985018.py:lookup_icd_codes_for_disease:27)\n",
      "2025-03-30 21:11:23,552 - micro - MainProcess - INFO     ICD-10 Codes Found for 'Hidradenitis Suppurativa': ['L73.2'] (1126985018.py:lookup_icd_codes_for_disease:42)\n",
      "2025-03-30 21:11:23,556 - micro - MainProcess - INFO     [Attempt 1] Fetching ICD-10 codes for disease: 'Juvenile Idiopathic Arthritis' with params: {'sf': 'code,name', 'terms': 'Juvenile Idiopathic Arthritis', 'maxList': 3} (1126985018.py:lookup_icd_codes_for_disease:27)\n",
      "2025-03-30 21:11:23,796 - micro - MainProcess - WARNING  No valid ICD-10 codes found for 'Juvenile Idiopathic Arthritis'. (1126985018.py:lookup_icd_codes_for_disease:45)\n",
      "2025-03-30 21:11:23,799 - micro - MainProcess - INFO     [Attempt 1] Fetching ICD-10 codes for disease: 'Plaque Psoriasis' with params: {'sf': 'code,name', 'terms': 'Plaque Psoriasis', 'maxList': 3} (1126985018.py:lookup_icd_codes_for_disease:27)\n",
      "2025-03-30 21:11:23,927 - micro - MainProcess - WARNING  No valid ICD-10 codes found for 'Plaque Psoriasis'. (1126985018.py:lookup_icd_codes_for_disease:45)\n",
      "2025-03-30 21:11:23,930 - micro - MainProcess - INFO     [Attempt 1] Fetching ICD-10 codes for disease: 'Psoriatic Arthritis' with params: {'sf': 'code,name', 'terms': 'Psoriatic Arthritis', 'maxList': 3} (1126985018.py:lookup_icd_codes_for_disease:27)\n",
      "2025-03-30 21:11:24,067 - micro - MainProcess - INFO     ICD-10 Codes Found for 'Psoriatic Arthritis': ['L40.52'] (1126985018.py:lookup_icd_codes_for_disease:42)\n",
      "2025-03-30 21:11:24,070 - micro - MainProcess - INFO     [Attempt 1] Fetching ICD-10 codes for disease: 'Rheumatoid Arthritis' with params: {'sf': 'code,name', 'terms': 'Rheumatoid Arthritis', 'maxList': 3} (1126985018.py:lookup_icd_codes_for_disease:27)\n",
      "2025-03-30 21:11:24,193 - micro - MainProcess - INFO     ICD-10 Codes Found for 'Rheumatoid Arthritis': ['M06.9', 'M05.9', 'M06.08'] (1126985018.py:lookup_icd_codes_for_disease:42)\n",
      "2025-03-30 21:11:24,195 - micro - MainProcess - INFO     [Attempt 1] Fetching ICD-10 codes for disease: 'Ulcerative Colitis' with params: {'sf': 'code,name', 'terms': 'Ulcerative Colitis', 'maxList': 3} (1126985018.py:lookup_icd_codes_for_disease:27)\n",
      "2025-03-30 21:11:24,320 - micro - MainProcess - INFO     ICD-10 Codes Found for 'Ulcerative Colitis': ['K51.80', 'K51.813', 'K51.814'] (1126985018.py:lookup_icd_codes_for_disease:42)\n",
      "2025-03-30 21:11:24,322 - micro - MainProcess - INFO     [Attempt 1] Fetching ICD-10 codes for disease: 'Uveitis' with params: {'sf': 'code,name', 'terms': 'Uveitis', 'maxList': 3} (1126985018.py:lookup_icd_codes_for_disease:27)\n",
      "2025-03-30 21:11:24,440 - micro - MainProcess - INFO     ICD-10 Codes Found for 'Uveitis': ['H44.133', 'H44.131', 'H44.132'] (1126985018.py:lookup_icd_codes_for_disease:42)\n",
      "2025-03-30 21:11:24,443 - micro - MainProcess - INFO     [Attempt 1] Fetching ICD-10 codes for disease: 'Non-radiographic axial spondyloarthritis' with params: {'sf': 'code,name', 'terms': 'Non-radiographic axial spondyloarthritis', 'maxList': 3} (1126985018.py:lookup_icd_codes_for_disease:27)\n",
      "2025-03-30 21:11:24,572 - micro - MainProcess - INFO     ICD-10 Codes Found for 'Non-radiographic axial spondyloarthritis': ['M45.A1', 'M45.A2', 'M45.A3'] (1126985018.py:lookup_icd_codes_for_disease:42)\n",
      "2025-03-30 21:11:24,574 - micro - MainProcess - INFO     [Attempt 1] Fetching ICD-10 codes for disease: 'Reactive Arthritis' with params: {'sf': 'code,name', 'terms': 'Reactive Arthritis', 'maxList': 3} (1126985018.py:lookup_icd_codes_for_disease:27)\n",
      "2025-03-30 21:11:24,819 - micro - MainProcess - WARNING  No valid ICD-10 codes found for 'Reactive Arthritis'. (1126985018.py:lookup_icd_codes_for_disease:45)\n",
      "2025-03-30 21:11:24,822 - micro - MainProcess - INFO     [Attempt 1] Fetching ICD-10 codes for disease: 'Inflammatory Bowel Disease-associated arthritis' with params: {'sf': 'code,name', 'terms': 'Inflammatory Bowel Disease-associated arthritis', 'maxList': 3} (1126985018.py:lookup_icd_codes_for_disease:27)\n",
      "2025-03-30 21:11:24,946 - micro - MainProcess - WARNING  No valid ICD-10 codes found for 'Inflammatory Bowel Disease-associated arthritis'. (1126985018.py:lookup_icd_codes_for_disease:45)\n",
      "2025-03-30 21:11:24,949 - micro - MainProcess - INFO     [Attempt 1] Fetching RxNorm ID for drug: 'Abrilada' with params: {'name': 'Abrilada'} (1126985018.py:lookup_drug_details:71)\n",
      "2025-03-30 21:11:25,563 - micro - MainProcess - INFO     RxNorm IDs Found for 'Abrilada': ['2668066'] (1126985018.py:lookup_drug_details:86)\n",
      "2025-03-30 21:11:25,565 - micro - MainProcess - INFO     [Attempt 1] Fetching RxNorm ID for drug: 'Adalimumab-aacf' with params: {'name': 'Adalimumab-aacf'} (1126985018.py:lookup_drug_details:71)\n",
      "2025-03-30 21:11:25,714 - micro - MainProcess - INFO     RxNorm IDs Found for 'Adalimumab-aacf': ['2640849'] (1126985018.py:lookup_drug_details:86)\n",
      "2025-03-30 21:11:25,719 - micro - MainProcess - INFO     [Attempt 1] Fetching RxNorm ID for drug: 'Adalimumab-adaz' with params: {'name': 'Adalimumab-adaz'} (1126985018.py:lookup_drug_details:71)\n",
      "2025-03-30 21:11:25,850 - micro - MainProcess - INFO     RxNorm IDs Found for 'Adalimumab-adaz': ['2641642'] (1126985018.py:lookup_drug_details:86)\n",
      "2025-03-30 21:11:25,854 - micro - MainProcess - INFO     [Attempt 1] Fetching RxNorm ID for drug: 'Adalimumab-adbm' with params: {'name': 'Adalimumab-adbm'} (1126985018.py:lookup_drug_details:71)\n",
      "2025-03-30 21:11:25,989 - micro - MainProcess - INFO     RxNorm IDs Found for 'Adalimumab-adbm': ['2103477'] (1126985018.py:lookup_drug_details:86)\n",
      "2025-03-30 21:11:25,992 - micro - MainProcess - INFO     [Attempt 1] Fetching RxNorm ID for drug: 'Adalimumab-fkjp' with params: {'name': 'Adalimumab-fkjp'} (1126985018.py:lookup_drug_details:71)\n",
      "2025-03-30 21:11:26,124 - micro - MainProcess - INFO     RxNorm IDs Found for 'Adalimumab-fkjp': ['2640402'] (1126985018.py:lookup_drug_details:86)\n",
      "2025-03-30 21:11:26,126 - micro - MainProcess - INFO     [Attempt 1] Fetching RxNorm ID for drug: 'Adalimumab-ryvk' with params: {'name': 'Adalimumab-ryvk'} (1126985018.py:lookup_drug_details:71)\n",
      "2025-03-30 21:11:26,254 - micro - MainProcess - INFO     RxNorm IDs Found for 'Adalimumab-ryvk': ['2677776'] (1126985018.py:lookup_drug_details:86)\n",
      "2025-03-30 21:11:26,256 - micro - MainProcess - INFO     [Attempt 1] Fetching RxNorm ID for drug: 'Amjevita' with params: {'name': 'Amjevita'} (1126985018.py:lookup_drug_details:71)\n",
      "2025-03-30 21:11:26,415 - micro - MainProcess - INFO     RxNorm IDs Found for 'Amjevita': ['2627713'] (1126985018.py:lookup_drug_details:86)\n",
      "2025-03-30 21:11:26,417 - micro - MainProcess - INFO     [Attempt 1] Fetching RxNorm ID for drug: 'Cyltezo' with params: {'name': 'Cyltezo'} (1126985018.py:lookup_drug_details:71)\n",
      "2025-03-30 21:11:26,547 - micro - MainProcess - INFO     RxNorm IDs Found for 'Cyltezo': ['2640885'] (1126985018.py:lookup_drug_details:86)\n",
      "2025-03-30 21:11:26,549 - micro - MainProcess - INFO     [Attempt 1] Fetching RxNorm ID for drug: 'Hadlima' with params: {'name': 'Hadlima'} (1126985018.py:lookup_drug_details:71)\n",
      "2025-03-30 21:11:26,674 - micro - MainProcess - INFO     RxNorm IDs Found for 'Hadlima': ['2640296'] (1126985018.py:lookup_drug_details:86)\n",
      "2025-03-30 21:11:26,676 - micro - MainProcess - INFO     [Attempt 1] Fetching RxNorm ID for drug: 'Hulio' with params: {'name': 'Hulio'} (1126985018.py:lookup_drug_details:71)\n",
      "2025-03-30 21:11:26,804 - micro - MainProcess - INFO     RxNorm IDs Found for 'Hulio': ['2640406'] (1126985018.py:lookup_drug_details:86)\n",
      "2025-03-30 21:11:26,805 - micro - MainProcess - INFO     [Attempt 1] Fetching RxNorm ID for drug: 'Humira' with params: {'name': 'Humira'} (1126985018.py:lookup_drug_details:71)\n",
      "2025-03-30 21:11:26,948 - micro - MainProcess - INFO     RxNorm IDs Found for 'Humira': ['353484'] (1126985018.py:lookup_drug_details:86)\n",
      "2025-03-30 21:11:26,952 - micro - MainProcess - INFO     [Attempt 1] Fetching RxNorm ID for drug: 'Hyrimoz' with params: {'name': 'Hyrimoz'} (1126985018.py:lookup_drug_details:71)\n",
      "2025-03-30 21:11:27,134 - micro - MainProcess - INFO     RxNorm IDs Found for 'Hyrimoz': ['2641646'] (1126985018.py:lookup_drug_details:86)\n",
      "2025-03-30 21:11:27,137 - micro - MainProcess - INFO     [Attempt 1] Fetching RxNorm ID for drug: 'Idacio' with params: {'name': 'Idacio'} (1126985018.py:lookup_drug_details:71)\n",
      "2025-03-30 21:11:27,296 - micro - MainProcess - INFO     RxNorm IDs Found for 'Idacio': ['2640853'] (1126985018.py:lookup_drug_details:86)\n",
      "2025-03-30 21:11:27,299 - micro - MainProcess - INFO     [Attempt 1] Fetching RxNorm ID for drug: 'Simlandi' with params: {'name': 'Simlandi'} (1126985018.py:lookup_drug_details:71)\n",
      "2025-03-30 21:11:27,488 - micro - MainProcess - INFO     RxNorm IDs Found for 'Simlandi': ['2677782'] (1126985018.py:lookup_drug_details:86)\n",
      "2025-03-30 21:11:27,490 - micro - MainProcess - INFO     [Attempt 1] Fetching RxNorm ID for drug: 'Yuflyma' with params: {'name': 'Yuflyma'} (1126985018.py:lookup_drug_details:71)\n",
      "2025-03-30 21:11:27,690 - micro - MainProcess - INFO     RxNorm IDs Found for 'Yuflyma': ['2639653'] (1126985018.py:lookup_drug_details:86)\n",
      "2025-03-30 21:11:27,693 - micro - MainProcess - INFO     [Attempt 1] Fetching RxNorm ID for drug: 'Yusimry' with params: {'name': 'Yusimry'} (1126985018.py:lookup_drug_details:71)\n",
      "2025-03-30 21:11:27,884 - micro - MainProcess - INFO     RxNorm IDs Found for 'Yusimry': ['2640391'] (1126985018.py:lookup_drug_details:86)\n"
     ]
    }
   ],
   "source": [
    "extract_policy_metadata = extract_policy_metadata(policy_raw_text_markdown.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'policy_name': 'Inflammatory Conditions - Adalimumab Products Prior Authorization Policy',\n",
       " 'payer_name': 'Cigna',\n",
       " 'drug_names': ['Abrilada',\n",
       "  'Adalimumab-aacf',\n",
       "  'Adalimumab-adaz',\n",
       "  'Adalimumab-adbm',\n",
       "  'Adalimumab-fkjp',\n",
       "  'Adalimumab-ryvk',\n",
       "  'Amjevita',\n",
       "  'Cyltezo',\n",
       "  'Hadlima',\n",
       "  'Hulio',\n",
       "  'Humira',\n",
       "  'Hyrimoz',\n",
       "  'Idacio',\n",
       "  'Simlandi',\n",
       "  'Yuflyma',\n",
       "  'Yusimry'],\n",
       " 'medical_specialties': ['Rheumatology',\n",
       "  'Dermatology',\n",
       "  'Gastroenterology',\n",
       "  'Ophthalmology'],\n",
       " 'indications_diseases': ['Ankylosing Spondylitis',\n",
       "  \"Crohn's Disease\",\n",
       "  'Hidradenitis Suppurativa',\n",
       "  'Juvenile Idiopathic Arthritis',\n",
       "  'Plaque Psoriasis',\n",
       "  'Psoriatic Arthritis',\n",
       "  'Rheumatoid Arthritis',\n",
       "  'Ulcerative Colitis',\n",
       "  'Uveitis',\n",
       "  'Non-radiographic axial spondyloarthritis',\n",
       "  'Reactive Arthritis',\n",
       "  'Inflammatory Bowel Disease-associated arthritis'],\n",
       " 'covered_diseases_icd_codes': ['M08.1',\n",
       "  'M45.6',\n",
       "  'M45.2',\n",
       "  'K50.90',\n",
       "  'K50.913',\n",
       "  'K50.914',\n",
       "  'L73.2',\n",
       "  'L40.52',\n",
       "  'M06.9',\n",
       "  'M05.9',\n",
       "  'M06.08',\n",
       "  'K51.80',\n",
       "  'K51.813',\n",
       "  'K51.814',\n",
       "  'H44.133',\n",
       "  'H44.131',\n",
       "  'H44.132',\n",
       "  'M45.A1',\n",
       "  'M45.A2',\n",
       "  'M45.A3'],\n",
       " 'covered_drug_codes': ['2668066',\n",
       "  '2640849',\n",
       "  '2641642',\n",
       "  '2103477',\n",
       "  '2640402',\n",
       "  '2677776',\n",
       "  '2627713',\n",
       "  '2640885',\n",
       "  '2640296',\n",
       "  '2640406',\n",
       "  '353484',\n",
       "  '2641646',\n",
       "  '2640853',\n",
       "  '2677782',\n",
       "  '2639653',\n",
       "  '2640391']}"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "extract_policy_metadata"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. **Chunk Documents**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_text_splitters import MarkdownHeaderTextSplitter\n",
    "\n",
    "headers_to_split_on = [\n",
    "    (\"#\", \"Header 1\"),\n",
    "    (\"##\", \"Header 2\"),\n",
    "    (\"###\", \"Header 3\"),\n",
    "    (\"####\", \"Header 4\")\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "splitter = MarkdownHeaderTextSplitter(headers_to_split_on=headers_to_split_on)\n",
    "chunks = splitter.split_text(policy_raw_text_markdown.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. **Vectorize, Add Metadata, Index**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "search_client = SearchClient(\n",
    "    endpoint=endpoint,\n",
    "    index_name=os.environ[\"AZURE_SEARCH_INDEX_NAME_EMPLOYEES\"],\n",
    "    credential=AzureKeyCredential(os.environ[\"AZURE_SEARCH_ADMIN_KEY\"]),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the service endpoint and API key from the environment\n",
    "# Create an SDK client\n",
    "import openai\n",
    "from openai import AzureOpenAI\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "openai.api_key = os.environ[\"AZURE_AOAI_API_KEY\"]\n",
    "openai.api_base = os.environ[\"AZURE_AOAI_API_ENDPOINT\"]\n",
    "openai.api_type = \"azure\"\n",
    "openai.api_version = \"2023-05-15\"\n",
    "\n",
    "model = os.environ[\"AZURE_AOAI_EMBEDDING_DEPLOYMENT_ID\"]\n",
    "\n",
    "client = AzureOpenAI(\n",
    "    api_version=openai.api_version,\n",
    "    azure_endpoint=openai.api_base,\n",
    "    api_key=openai.api_key,\n",
    ")\n",
    "\n",
    "# This is in characters and there is an avg of 4 chars / token\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=500,\n",
    "    chunk_overlap=150,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = 100  # max batch size (number of docs) to upload at a time\n",
    "total_docs_uploaded = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Documents to Upload: 6\n",
      "Uploading batch of 6 documents...\n",
      "Upload of batch of 6 documents succeeded.\n"
     ]
    }
   ],
   "source": [
    "# Split up a list into chunks - this is used to ensure a limited number of items sent to Azure AI Search\n",
    "def divide_chunks(l, n):\n",
    "    for i in range(0, len(l), n):\n",
    "        yield l[i : i + n]\n",
    "\n",
    "\n",
    "# Function to generate embeddings for content fields, also used for query embeddings\n",
    "@retry(wait=wait_random_exponential(min=1, max=20), stop=stop_after_attempt(6))\n",
    "def generate_embeddings(text):\n",
    "    response = client.embeddings.create(input=text, model=model)\n",
    "    return json.loads(response.model_dump_json())[\"data\"][0][\"embedding\"]\n",
    "\n",
    "\n",
    "chunked_content_docs = []\n",
    "sfc_counter = 0\n",
    "for sfc_counter in range(len(final_data_for_indexing)):\n",
    "    chunked_content = text_splitter.split_text(\n",
    "        final_data_for_indexing[sfc_counter][\"content\"]\n",
    "    )\n",
    "    chunk_counter = 0\n",
    "    for cc in chunked_content:\n",
    "        json_data = copy.deepcopy(final_data_for_indexing[sfc_counter])\n",
    "        json_data[\"content\"] = chunked_content[chunk_counter]\n",
    "        json_data[\"content_vector\"] = generate_embeddings(json_data[\"content\"])\n",
    "        json_data[\"ParentId\"] = f\"{json_data['ParentId']}\"\n",
    "        json_data[\"id\"] = f\"{json_data['ParentId']}_{chunk_counter}\"\n",
    "        chunk_counter += 1\n",
    "        chunked_content_docs.append(json_data)\n",
    "    sfc_counter += 1\n",
    "\n",
    "total_docs = len(chunked_content_docs)\n",
    "total_docs_uploaded += total_docs\n",
    "print(f\"Total Documents to Upload: {total_docs}\")\n",
    "\n",
    "for documents_chunk in divide_chunks(chunked_content_docs, n):\n",
    "    try:\n",
    "        print(f\"Uploading batch of {len(documents_chunk)} documents...\")\n",
    "        result = search_client.upload_documents(documents=documents_chunk)\n",
    "        # Check if all documents in the batch were uploaded successfully\n",
    "        if all(res.succeeded for res in result):\n",
    "            print(f\"Upload of batch of {len(documents_chunk)} documents succeeded.\")\n",
    "        else:\n",
    "            print(\"Some documents in the batch were not uploaded successfully.\")\n",
    "    except Exception as ex:\n",
    "        print(\"Error in multiple documents upload: \", ex)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Retrieval Data from Azure AI search"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before executing this notebook, please review the notebook `04-retrieval.ipynb` for a better understanding of the process. \n",
    "\n",
    ">%pip install azure-search-documents==11.4.0b10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from azure.core.credentials import AzureKeyCredential\n",
    "from azure.search.documents import SearchClient\n",
    "from azure.search.documents.models import RawVectorQuery\n",
    "\n",
    "from src.aoai.azure_open_ai import AzureOpenAIManager"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load environment variables from .env file\n",
    "load_dotenv()\n",
    "\n",
    "# Set up Azure Cognitive Search credentials\n",
    "service_endpoint = os.getenv(\"AZURE_AI_SEARCH_SERVICE_ENDPOINT\")\n",
    "key = os.getenv(\"AZURE_SEARCH_ADMIN_KEY\")\n",
    "credential = AzureKeyCredential(key)\n",
    "\n",
    "# Define the name of the Azure Search index\n",
    "# This is the index where your data is stored in Azure Search\n",
    "index_name = os.getenv(\"AZURE_SEARCH_INDEX_NAME_EMPLOYEES\")\n",
    "\n",
    "# Set up the Azure Search client with the specified index\n",
    "# This prepares the client to interact with the Azure Search service\n",
    "search_client = SearchClient(service_endpoint, index_name, credential=credential)\n",
    "\n",
    "# Set up the Azure Search client with the specified index\n",
    "# This prepares the client to interact with the Azure Search service\n",
    "search_client = SearchClient(service_endpoint, index_name, credential=credential)\n",
    "\n",
    "embedding_aoai_deployment_model = \"foundational-canadaeast-ada\"\n",
    "aoai_client = AzureOpenAIManager(embedding_model_name=embedding_aoai_deployment_model)\n",
    "\n",
    "search_query = \"has showcased remarkable expertise and dedication\"\n",
    "search_vector = aoai_client.generate_embedding(search_query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ID: E04105_Employee_Sample_Data_0, Age: 59, Annual Salary: 99975.0, score: 0.03333333507180214, reranker: 3.553462028503418. Theodore has showcased remarkable expertise and dedication in his role as Technical Architect. His c\n",
      "ID: E02387_Employee_Sample_Data_0, Age: 55, Annual Salary: 141604.0, score: 0.032786883413791656, reranker: 1.8165210485458374. Emily has consistently demonstrated exceptional leadership and technical expertise. Her ability to d\n",
      "ID: E02387_Employee_Sample_Data_1, Age: 55, Annual Salary: 141604.0, score: 0.03125763311982155, reranker: 1.536381721496582. on time and within budget. Emily's strengths lie in her strategic thinking, leadership, and technica\n",
      "ID: E02387_Employee_Sample_Data_2, Age: 55, Annual Salary: 141604.0, score: 0.0314980149269104, reranker: 1.4992340803146362. executive leadership training and participating in conferences focusing on emerging technologies. Em\n",
      "ID: E04105_Employee_Sample_Data_1, Age: 59, Annual Salary: 99975.0, score: 0.032258063554763794, reranker: 1.4957873821258545. architecture, resulting in a 20% improvement in system efficiency and a significant reduction in dow\n"
     ]
    }
   ],
   "source": [
    "# hybrid retrieval + rerank\n",
    "r = search_client.search(\n",
    "    search_query,\n",
    "    top=5,\n",
    "    vector_queries=[\n",
    "        RawVectorQuery(vector=search_vector, k=50, fields=\"content_vector\")\n",
    "    ],\n",
    "    query_type=\"semantic\",\n",
    "    semantic_configuration_name=\"combined-index-fields-semantic-config\",\n",
    "    query_language=\"en-us\",\n",
    ")\n",
    "\n",
    "for doc in r:\n",
    "    content = doc[\"content\"].replace(\"\\n\", \" \")[:100]\n",
    "    print(\n",
    "        f\"ID: {doc['id']}, Age: {doc['Age']}, Annual Salary: {doc['AnnualSalary']}, score: {doc['@search.score']}, reranker: {doc['@search.reranker_score']}. {content}\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example 1 - Filtering Data Based on Age and Annual Salary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "expression `Age gt 58 and AnnualSalary lt 100000` \n",
    "\n",
    "The expression uses the 'gt' (greater than) and 'lt' (less than) operators to filter documents based on the values of the 'Age' and 'AnnualSalary' fields.\n",
    "\n",
    "\"Age gt 58\" returns documents where the 'Age' field is greater than 58.\n",
    "\"AnnualSalary lt 100000\" returns documents where the 'AnnualSalary' field is less than 100000.\n",
    "\n",
    "The 'and' operator is used to combine these two conditions, so the expression only returns documents where both conditions are true. That is, it returns documents where the 'Age' field is greater than 58 and the 'AnnualSalary' field is less than 100000.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ID: E04105_Employee_Sample_Data_0, Age: 59, Annual Salary: 99975.0, score: 0.03333333507180214, reranker: 3.553462028503418. Theodore has showcased remarkable expertise and dedication in his role as Technical Architect. His c\n",
      "ID: E04105_Employee_Sample_Data_1, Age: 59, Annual Salary: 99975.0, score: 0.032786883413791656, reranker: 1.4954043626785278. architecture, resulting in a 20% improvement in system efficiency and a significant reduction in dow\n",
      "ID: E04105_Employee_Sample_Data_2, Age: 59, Annual Salary: 99975.0, score: 0.032258063554763794, reranker: 1.4904258251190186. advanced certifications in emerging technologies relevant to manufacturing IT systems and attending \n"
     ]
    }
   ],
   "source": [
    "r = search_client.search(\n",
    "    search_query,\n",
    "    top=5,\n",
    "    vector_queries=[\n",
    "        RawVectorQuery(vector=search_vector, k=50, fields=\"content_vector\")\n",
    "    ],\n",
    "    query_type=\"semantic\",\n",
    "    semantic_configuration_name=\"combined-index-fields-semantic-config\",\n",
    "    query_language=\"en-us\",\n",
    "    filter=\"Age gt 58 and AnnualSalary lt 100000\",  # Add filter expression\n",
    ")\n",
    "\n",
    "for doc in r:\n",
    "    content = doc[\"content\"].replace(\"\\n\", \" \")[:1000000]\n",
    "    print(\n",
    "        f\"ID: {doc['id']}, Age: {doc['Age']}, Annual Salary: {doc['AnnualSalary']}, score: {doc['@search.score']}, reranker: {doc['@search.reranker_score']}. {content}\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Indexing Content from Multiple Sources (Azure SQL DB and Blob Storage) Using Azure AI search Indexer Option"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.indexers.sql_Indexing import AzureSQLManager\n",
    "\n",
    "DATABASE = \"dev-sql-server\"\n",
    "az_sql_client = AzureSQLManager(DATABASE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished dropping table 'foodreview' if it existed. This ensures we are starting with a clean slate.\n",
      "Finished creating table 'foodreview'. This table will store our food review data.\n",
      "Finished creating index 'idx_Id' on table 'foodreview'. This index will improve the performance of queries that filter by the 'Id' field.\n"
     ]
    }
   ],
   "source": [
    "table_name = \"foodreview\"\n",
    "\n",
    "# Drop previous table of same name if one exists\n",
    "az_sql_client.execute(f\"DROP TABLE IF EXISTS {table_name};\")\n",
    "print(\n",
    "    f\"Finished dropping table '{table_name}' if it existed. This ensures we are starting with a clean slate.\"\n",
    ")\n",
    "\n",
    "# Create a table\n",
    "az_sql_client.execute(\n",
    "    f\"\"\"\n",
    "               CREATE TABLE {table_name} \n",
    "               (Id int NOT NULL, \n",
    "               CONSTRAINT PK_{table_name}_Id PRIMARY KEY CLUSTERED (Id), \n",
    "               ProductId text, \n",
    "               UserId text, \n",
    "               ProfileName text, \n",
    "               HelpfulnessNumerator integer, \n",
    "               HelpfulnessDenominator integer, \n",
    "               Score integer, \n",
    "               Time bigint, \n",
    "               Summary text, \n",
    "               Recommnedation NVARCHAR(MAX));\n",
    "               \"\"\"\n",
    ")\n",
    "print(\n",
    "    f\"Finished creating table '{table_name}'. This table will store our food review data.\"\n",
    ")\n",
    "\n",
    "# Create an index\n",
    "az_sql_client.execute(f\"CREATE INDEX idx_Id ON {table_name}(Id);\")\n",
    "print(\n",
    "    f\"Finished creating index 'idx_Id' on table '{table_name}'. This index will improve the performance of queries that filter by the 'Id' field.\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Enable change tracking\n",
    "\n",
    "This allows us to automatically update the index when changes are made to the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:root:Error executing query: ALTER DATABASE [dev-sql-server] SET CHANGE_TRACKING = ON (CHANGE_RETENTION = 2 DAYS, AUTO_CLEANUP = ON)\n",
      "Error Message: ('42000', \"[42000] [Microsoft][ODBC Driver 18 for SQL Server][SQL Server]Change tracking is already enabled for database 'dev-sql-server'. (5088) (SQLExecDirectW); [42000] [Microsoft][ODBC Driver 18 for SQL Server][SQL Server]ALTER DATABASE statement failed. (5069)\")\n",
      "Traceback: Traceback (most recent call last):\n",
      "  File \"c:\\Users\\pablosal\\Desktop\\gbbai-azure-ai-search-indexing\\src\\indexers\\sql_Indexing.py\", line 78, in execute\n",
      "    self.cursor.execute(query)\n",
      "pyodbc.ProgrammingError: ('42000', \"[42000] [Microsoft][ODBC Driver 18 for SQL Server][SQL Server]Change tracking is already enabled for database 'dev-sql-server'. (5088) (SQLExecDirectW); [42000] [Microsoft][ODBC Driver 18 for SQL Server][SQL Server]ALTER DATABASE statement failed. (5069)\")\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('42000', \"[42000] [Microsoft][ODBC Driver 18 for SQL Server][SQL Server]Change tracking is already enabled for database 'dev-sql-server'. (5088) (SQLExecDirectW); [42000] [Microsoft][ODBC Driver 18 for SQL Server][SQL Server]ALTER DATABASE statement failed. (5069)\")\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    # Commit any active transactions\n",
    "    az_sql_client.cursor.commit()\n",
    "\n",
    "    # Enable change tracking at the database level\n",
    "    az_sql_client.execute(\n",
    "        f\"ALTER DATABASE [{DATABASE}] SET CHANGE_TRACKING = ON (CHANGE_RETENTION = 2 DAYS, AUTO_CLEANUP = ON)\"\n",
    "    )\n",
    "except Exception as e:\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    # Enable change tracking on a specific table.\n",
    "    # This is necessary to keep track of changes made to this specific table.\n",
    "    # The TRACK_COLUMNS_UPDATED option is turned on, allowing the system to keep track of which columns were updated.\n",
    "    az_sql_client.execute(\n",
    "        f\"ALTER TABLE {table_name} ENABLE CHANGE_TRACKING WITH (TRACK_COLUMNS_UPDATED = ON)\"\n",
    "    )\n",
    "except Exception as e:\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "try:\n",
    "    all_reviews = pd.read_csv(\"utils/data/Reviews_small.csv\")\n",
    "    sample_reviews = all_reviews[:1000].copy()\n",
    "    product_text_reviews = sample_reviews[[\"ProductId\", \"Text\"]].copy()\n",
    "    sample_reviews.drop(columns=[\"Text\"], inplace=True)\n",
    "    product_text_reviews.to_csv(\"utils/data/Reviews_text.csv\", index=False)\n",
    "except FileNotFoundError:\n",
    "    print(\"File not found.\")\n",
    "except Exception as e:\n",
    "    print(f\"Error: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Uploading Data to SQL Server"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 30\n",
    "batches = [\n",
    "    sample_reviews[i : i + batch_size]\n",
    "    for i in range(0, len(sample_reviews), batch_size)\n",
    "]\n",
    "\n",
    "# Iterate over each batch and insert or update the data in the database\n",
    "for batch in batches:\n",
    "    # Convert the batch dataframe to a list of tuples for bulk insertion\n",
    "    rows = [tuple(row) for row in batch.itertuples(index=False)]\n",
    "\n",
    "    # Define the SQL query for bulk insertion or update\n",
    "    query = f\"\"\"\n",
    "    MERGE INTO {table_name} AS Target\n",
    "    USING (VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?)) AS Source (Id, ProductId, UserId, ProfileName, HelpfulnessNumerator, HelpfulnessDenominator, Score, Time, Summary)\n",
    "    ON CAST(Target.ProductId AS NVARCHAR(MAX)) = Source.ProductId\n",
    "    WHEN MATCHED THEN \n",
    "        UPDATE SET UserId = Source.UserId, ProfileName = Source.ProfileName, HelpfulnessNumerator = Source.HelpfulnessNumerator, HelpfulnessDenominator = Source.HelpfulnessDenominator, Score = Source.Score, Time = Source.Time, Summary = Source.Summary\n",
    "    WHEN NOT MATCHED THEN\n",
    "        INSERT (Id, ProductId, UserId, ProfileName, HelpfulnessNumerator, HelpfulnessDenominator, Score, Time, Summary)\n",
    "        VALUES (Source.Id, Source.ProductId, Source.UserId, Source.ProfileName, Source.HelpfulnessNumerator, Source.HelpfulnessDenominator, Source.Score, Source.Time, Source.Summary);\n",
    "    \"\"\"\n",
    "    az_sql_client.cursor.executemany(query, rows)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(29,)]\n"
     ]
    }
   ],
   "source": [
    "# Execute the SELECT statement\n",
    "try:\n",
    "    data = az_sql_client.execute_and_fetch(f\"SELECT count(Id) FROM {table_name};\")\n",
    "    print(data)\n",
    "except Exception as e:\n",
    "    print(f\"Error executing SELECT statement: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(1, 'B001E4KFG0', 'A3SGXH7AUHU8GW', 'delmartian', 1, 1, 5, 1303862400, 'Good Quality Dog Food', None),\n",
       " (2, 'B00813GRG4', 'A1D87F6ZCVE5NK', 'dll pa', 0, 0, 1, 1346976000, 'Not as Advertised', None),\n",
       " (3, 'B000LQOCH0', 'ABXLMWJIXXAIN', 'Natalia Corres \"Natalia Corres\"', 1, 1, 4, 1219017600, '\"Delight\" says it all', None),\n",
       " (4, 'B000UA0QIQ', 'A395BORC6FGVXV', 'Karl', 3, 3, 2, 1307923200, 'Cough Medicine', None),\n",
       " (5, 'B006K2ZZ7K', 'A3JRGQVEQN31IQ', 'Pamela G. Williams', 0, 0, 5, 1336003200, 'Wonderful, tasty taffy', None)]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = az_sql_client.execute_and_fetch(f\"SELECT * FROM {table_name};\")\n",
    "data[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Indexing Data from Blob Storage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.extractors.blob_data_extractors import AzureBlobDataExtractor\n",
    "\n",
    "blob_client = AzureBlobDataExtractor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_from_blob = blob_client.read_csv_from_blob(\n",
    "    \"Reviews_text.csv\", \"testretrieval\", sep=\",\", encoding=\"ISO-8859-1\", header=0\n",
    ")\n",
    "df_from_blob[\"ProductId\"] = df_from_blob[\"ProductId\"].astype(str)\n",
    "df_from_blob[\"Recommnedation\"] = df_from_blob[\"Text\"].astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 30\n",
    "batches = [\n",
    "    df_from_blob[i : i + batch_size] for i in range(0, len(df_from_blob), batch_size)\n",
    "]\n",
    "\n",
    "for batch in batches:\n",
    "    # Convert the batch dataframe to a list of tuples for bulk update\n",
    "    # Adjust the unpacking to match the actual structure of your DataFrame tuples\n",
    "    rows = [(text, product_id) for _, product_id, text, *_ in batch.itertuples()]\n",
    "\n",
    "    # Define the SQL query for bulk update\n",
    "    query = f\"UPDATE {table_name} SET Recommnedation = ? WHERE CAST(ProductId AS NVARCHAR(MAX)) = ?\"\n",
    "    az_sql_client.cursor.executemany(query, rows)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(1, 'B001E4KFG0', 'A3SGXH7AUHU8GW', 'delmartian', 1, 1, 5, 1303862400, 'Good Quality Dog Food', 'I have bought several of the Vitality canned dog food products and have found them all to be of good quality. The product looks more like a stew than a processed meat and it smells better. My Labrador is finicky and she appreciates this product better than  most.'),\n",
       " (2, 'B00813GRG4', 'A1D87F6ZCVE5NK', 'dll pa', 0, 0, 1, 1346976000, 'Not as Advertised', 'Product arrived labeled as Jumbo Salted Peanuts...the peanuts were actually small sized unsalted. Not sure if this was an error or if the vendor intended to represent the product as \"Jumbo\".'),\n",
       " (3, 'B000LQOCH0', 'ABXLMWJIXXAIN', 'Natalia Corres \"Natalia Corres\"', 1, 1, 4, 1219017600, '\"Delight\" says it all', 'This is a confection that has been around a few centuries.  It is a light, pillowy citrus gelatin with nuts - in this case Filberts. And it is cut into tiny squares and then liberally coated with powdered sugar.  And it is a tiny mouthful of heaven.  Not too chewy, and very flavorful.  I highly recommend this yummy treat.  If you are familiar with the story of C.S. Lewis\\' \"The Lion, The Witch, and The Wardrobe\" - this is the treat that seduces Edmund into selling out his Brother and Sisters to the Witch.'),\n",
       " (4, 'B000UA0QIQ', 'A395BORC6FGVXV', 'Karl', 3, 3, 2, 1307923200, 'Cough Medicine', 'If you are looking for the secret ingredient in Robitussin I believe I have found it.  I got this in addition to the Root Beer Extract I ordered (which was good) and made some cherry soda.  The flavor is very medicinal.'),\n",
       " (5, 'B006K2ZZ7K', 'A3JRGQVEQN31IQ', 'Pamela G. Williams', 0, 0, 5, 1336003200, 'Wonderful, tasty taffy', 'This taffy is so good.  It is very soft and chewy.  The flavors are amazing.  I would definitely recommend you buying it.  Very satisfying!!')]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = az_sql_client.execute_and_fetch(f\"SELECT * FROM {table_name};\")\n",
    "data[:5]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mediindexer",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
